# æ³¨æ„åŠ›çƒ­åŠ›å›¾å¯è§†åŒ–

è¿™ä¸ªæ¨¡å—æä¾›äº†ç”¨äºå¯è§†åŒ–å¤šæ¨¡æ€æ¨¡å‹æ³¨æ„åŠ›æƒé‡çš„å·¥å…·ï¼Œç‰¹åˆ«é€‚é…äº†Reason-RFTé¡¹ç›®çš„è½¨è¿¹æ¨ç†ä»»åŠ¡ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ¯ **æ–‡æœ¬æ³¨æ„åŠ›å¯è§†åŒ–**: æ˜¾ç¤ºæ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶å¯¹ä¸åŒtokençš„æ³¨æ„åŠ›åˆ†å¸ƒ
- ğŸ–¼ï¸ **å›¾åƒæ³¨æ„åŠ›å¯è§†åŒ–**: åœ¨åŸå§‹å›¾åƒä¸Šå åŠ æ³¨æ„åŠ›çƒ­åŠ›å›¾ï¼Œæ˜¾ç¤ºæ¨¡å‹å…³æ³¨çš„å›¾åƒåŒºåŸŸ
- ğŸ¬ **è§†é¢‘è¾“å‡º**: ç”ŸæˆåŒ…å«æ–‡æœ¬å’Œå›¾åƒæ³¨æ„åŠ›å¹¶æ’æ˜¾ç¤ºçš„è§†é¢‘
- ğŸ”§ **çµæ´»é…ç½®**: æ”¯æŒé€‰æ‹©ä¸åŒçš„æ³¨æ„åŠ›å±‚ã€èµ·å§‹çŸ­è¯­ç­‰å‚æ•°

## æ–‡ä»¶è¯´æ˜

- `visualizeHeatMap.py`: ä¸»è¦çš„å¯è§†åŒ–ä»£ç ï¼ŒåŒ…å«æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½
- `run_attention_visualization.py`: ä½¿ç”¨ç¤ºä¾‹è„šæœ¬
- `README_attention_visualization.md`: æœ¬è¯´æ˜æ–‡æ¡£

## å®‰è£…ä¾èµ–

ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹PythonåŒ…ï¼š

```bash
pip install torch transformers pillow imageio tqdm numpy
```

## ä½¿ç”¨æ–¹æ³•

### æ–¹æ³•1: ä½¿ç”¨ç¤ºä¾‹è„šæœ¬

```bash
python run_attention_visualization.py
```

åœ¨è¿è¡Œä¹‹å‰ï¼Œè¯·ä¿®æ”¹è„šæœ¬ä¸­çš„è·¯å¾„é…ç½®ï¼š

```python
model_path = "your/model/path"  # ä½ çš„æ¨¡å‹è·¯å¾„
benchmark_json = "your/dataset.json"  # æ•°æ®é›†JSONæ–‡ä»¶è·¯å¾„
image_dir = "your/image/directory"  # å›¾åƒç›®å½•è·¯å¾„
```

### æ–¹æ³•2: ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°

```bash
python visualizeHeatMap.py \
    --model_path "checkpoints/your_model" \
    --benchmark_json "trajDataJsonsDirty/sft/chengdu/pointLabel13/test-ours.json" \
    --image_dir "VLM" \
    --sample_idx 0 \
    --layer_idx 20 \
    --start_phrase "Based on the following requirements"
```

### æ–¹æ³•3: åœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from visualizeHeatMap import AttentionVisualizer

# åˆ›å»ºå¯è§†åŒ–å™¨
visualizer = AttentionVisualizer("your/model/path")

# åŠ è½½æ ·æœ¬æ•°æ®
sample, images = visualizer.load_sample_data(
    "your/dataset.json", 
    "your/image/dir", 
    sample_idx=0
)

# åˆ›å»ºæ³¨æ„åŠ›å¯è§†åŒ–
output_path = visualizer.create_attention_visualization_for_sample(
    sample=sample,
    images=images,
    layer_idx=20,
    start_phrase="Based on the following requirements"
)
```

## å‚æ•°è¯´æ˜

### AttentionVisualizerç±»å‚æ•°

- `model_name_or_path`: æ¨¡å‹è·¯å¾„
- `max_image_num`: æœ€å¤§å›¾åƒæ•°é‡ï¼ˆé»˜è®¤2ï¼‰

### create_attention_visualization_for_sampleæ–¹æ³•å‚æ•°

- `sample`: æ•°æ®é›†æ ·æœ¬å­—å…¸
- `images`: PILå›¾åƒåˆ—è¡¨
- `layer_idx`: è¦å¯è§†åŒ–çš„æ³¨æ„åŠ›å±‚ç´¢å¼•ï¼ˆé»˜è®¤20ï¼‰
- `start_phrase`: å¼€å§‹å¯è§†åŒ–çš„æ–‡æœ¬çŸ­è¯­ï¼ˆé»˜è®¤"Based on the following requirements"ï¼‰

### å‘½ä»¤è¡Œå‚æ•°

- `--model_path`: æ¨¡å‹è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--benchmark_json`: åŸºå‡†æµ‹è¯•JSONæ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--image_dir`: å›¾åƒç›®å½•è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
- `--sample_idx`: è¦å¯è§†åŒ–çš„æ ·æœ¬ç´¢å¼•ï¼ˆé»˜è®¤0ï¼‰
- `--layer_idx`: è¦å¯è§†åŒ–çš„æ³¨æ„åŠ›å±‚ç´¢å¼•ï¼ˆé»˜è®¤20ï¼‰
- `--start_phrase`: å¼€å§‹å¯è§†åŒ–çš„çŸ­è¯­ï¼ˆé»˜è®¤"Based on the following requirements"ï¼‰

## è¾“å‡ºè¯´æ˜

### è§†é¢‘å†…å®¹

ç”Ÿæˆçš„è§†é¢‘åŒ…å«ä¸¤éƒ¨åˆ†ï¼š
1. **å·¦ä¾§**: æ–‡æœ¬æ³¨æ„åŠ›å¯è§†åŒ–
   - çº¢è‰²: ä½æ³¨æ„åŠ›æƒé‡
   - ç»¿è‰²: é«˜æ³¨æ„åŠ›æƒé‡
   - è“è‰²: å½“å‰æ­£åœ¨ç”Ÿæˆçš„token

2. **å³ä¾§**: å›¾åƒæ³¨æ„åŠ›å¯è§†åŒ–
   - åŸå§‹å›¾åƒä¸Šå åŠ ç½‘æ ¼
   - æ¯ä¸ªç½‘æ ¼å•å…ƒçš„é¢œè‰²è¡¨ç¤ºè¯¥åŒºåŸŸçš„æ³¨æ„åŠ›æƒé‡
   - çº¢è‰²åˆ°ç»¿è‰²çš„æ¸å˜è¡¨ç¤ºæ³¨æ„åŠ›å¼ºåº¦

### æ–‡ä»¶å‘½å

è¾“å‡ºæ–‡ä»¶æ ¼å¼ï¼š`combined_attention_visualization_layer{layer_idx}.mp4`

ä¾‹å¦‚ï¼š`combined_attention_visualization_layer20.mp4`

## æ•°æ®é›†æ ¼å¼

ä»£ç æœŸæœ›çš„æ•°æ®é›†æ ¼å¼ä¸Reason-RFTé¡¹ç›®ä¸€è‡´ï¼š

```json
[
    {
        "id": "sample_id",
        "image": ["path/to/image.png"],
        "problem": "é—®é¢˜æ–‡æœ¬",
        "solution": "ç­”æ¡ˆæ–‡æœ¬",
        "answer": "ç­”æ¡ˆæ–‡æœ¬"
    }
]
```

## æ³¨æ„äº‹é¡¹

1. **å†…å­˜ä½¿ç”¨**: ç”Ÿæˆæ³¨æ„åŠ›å¯è§†åŒ–éœ€è¦å¤§é‡å†…å­˜ï¼Œå»ºè®®åœ¨GPUä¸Šè¿è¡Œ
2. **æ¨¡å‹å…¼å®¹æ€§**: ä»£ç ä½¿ç”¨transformersåº“ç›´æ¥åŠ è½½æ¨¡å‹ï¼Œç¡®ä¿æ¨¡å‹æ”¯æŒæ³¨æ„åŠ›æƒé‡è¾“å‡º
3. **å›¾åƒæ ¼å¼**: æ”¯æŒå¸¸è§çš„å›¾åƒæ ¼å¼ï¼ˆPNG, JPGç­‰ï¼‰
4. **å­—ä½“æ”¯æŒ**: ä»£ç ä¼šè‡ªåŠ¨å°è¯•åŠ è½½ç­‰å®½å­—ä½“ï¼Œå¦‚æœå¤±è´¥ä¼šä½¿ç”¨é»˜è®¤å­—ä½“

## æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **æ¨¡å‹åŠ è½½å¤±è´¥**
   - æ£€æŸ¥æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®ä¿æ¨¡å‹æ”¯æŒæ³¨æ„åŠ›æƒé‡è¾“å‡º

2. **å†…å­˜ä¸è¶³**
   - å‡å°‘batch size
   - ä½¿ç”¨æ›´å°çš„å›¾åƒå°ºå¯¸
   - å‡å°‘max_new_tokenså‚æ•°

3. **å­—ä½“æ˜¾ç¤ºé—®é¢˜**
   - ä»£ç ä¼šè‡ªåŠ¨å°è¯•å¤šç§å­—ä½“
   - å¦‚æœä»æœ‰é—®é¢˜ï¼Œå¯ä»¥æ‰‹åŠ¨æŒ‡å®šå­—ä½“è·¯å¾„

4. **è§†é¢‘ç”Ÿæˆå¤±è´¥**
   - ç¡®ä¿å®‰è£…äº†imageioå’Œffmpeg
   - æ£€æŸ¥è¾“å‡ºç›®å½•çš„å†™å…¥æƒé™

### è°ƒè¯•æ¨¡å¼

åœ¨ä»£ç ä¸­æ·»åŠ è°ƒè¯•ä¿¡æ¯ï¼š

```python
import logging
logging.basicConfig(level=logging.DEBUG)

# åœ¨å¯è§†åŒ–å™¨åˆ›å»ºæ—¶æ·»åŠ è¯¦ç»†æ—¥å¿—
visualizer = AttentionVisualizer(model_path)
```

## æ‰©å±•åŠŸèƒ½

### è‡ªå®šä¹‰å¯è§†åŒ–æ ·å¼

å¯ä»¥ä¿®æ”¹`visualize_attention_step`å‡½æ•°æ¥è‡ªå®šä¹‰æ–‡æœ¬å¯è§†åŒ–æ ·å¼ï¼š

```python
# ä¿®æ”¹é¢œè‰²æ˜ å°„
red = int(255 * (1 - weight))
green = int(255 * weight)
blue = int(255 * weight * 0.5)  # æ·»åŠ è“è‰²åˆ†é‡
color = (red, green, blue)
```

### æ·»åŠ æ›´å¤šå±‚å¯è§†åŒ–

å¯ä»¥åŒæ—¶å¯è§†åŒ–å¤šä¸ªæ³¨æ„åŠ›å±‚ï¼š

```python
for layer_idx in [10, 20, 30]:
    output_path = visualizer.create_attention_visualization_for_sample(
        sample, images, layer_idx, start_phrase
    )
```

## è´¡çŒ®

æ¬¢è¿æäº¤é—®é¢˜å’Œæ”¹è¿›å»ºè®®ï¼ 